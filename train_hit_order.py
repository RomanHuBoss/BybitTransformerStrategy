import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import logging
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score

from dataset import load_train_features, load_train_labels_hitorder, HitOrderDataset
from model import HitOrderClassifier
from config import CFG

logging.basicConfig(level=logging.INFO, format='%(asctime)s [INFO] %(message)s')

def main():
    # 1Ô∏è‚É£ –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    logging.info(f"‚ö°Ô∏è –†–∞–±–æ—Ç–∞–µ–º –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ: {device}")

    # 2Ô∏è‚É£ –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏ –º–µ—Ç–∫–∏
    logging.info("üöÄ –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏ –º–µ—Ç–∫–∏ HitOrder...")
    X = load_train_features()
    y_full = load_train_labels_hitorder()

    # –í–∞–∂–Ω–æ: —Ç–µ–ø–µ—Ä—å —É –Ω–∞—Å 4 —Å—Ç–æ–ª–±—Ü–∞ ‚Äî –±–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –∫–æ–ª–æ–Ω–∫—É 'hit' (—Ç.–µ. –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å—Ç–æ–ª–±–µ—Ü)
    y = y_full[:, 3].astype(np.float32)

    # –ù–µ–º–Ω–æ–≥–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –∫–ª–∞—Å—Å–∞–º
    n_pos = np.sum(y == 1)
    n_neg = np.sum(y == 0)
    logging.info(f"‚úÖ –ë–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤: POS={n_pos}, NEG={n_neg}, POS%={n_pos / len(y):.3%}")

    # 3Ô∏è‚É£ –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ train/val
    X_train, X_val, y_train, y_val = train_test_split(
        X, y, test_size=CFG.train.val_size, shuffle=False
    )

    # 4Ô∏è‚É£ –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç—ã –∏ –∑–∞–≥—Ä—É–∑—á–∏–∫–∏
    train_dataset = HitOrderDataset(X_train, y_train)
    val_dataset = HitOrderDataset(X_val, y_val)

    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=CFG.train.batch_size, shuffle=True)
    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=CFG.train.batch_size)

    # 5Ô∏è‚É£ –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
    model = HitOrderClassifier(input_size=X.shape[1]).to(device)

    # 6Ô∏è‚É£ –§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å (BCELoss –ø—Ä–∏ —Å–ª–∞–±–æ–º –¥–∏—Å–±–∞–ª–∞–Ω—Å–µ ‚Äî –¥–æ–ø—É—Å—Ç–∏–º–æ, –∏–Ω–∞—á–µ –º–æ–∂–Ω–æ –ø–æ–∑–∂–µ –≤–≤–µ—Å—Ç–∏ BCEWithLogitsLoss + pos_weight)
    loss_fn = nn.BCELoss()
    optimizer = optim.AdamW(model.parameters(), lr=CFG.train.lr)

    # 7Ô∏è‚É£ Early stopping –∏ –ª–æ–≥–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
    best_val_loss = np.inf
    patience_counter = 0
    logging.info("üßÆ –°—Ç–∞—Ä—Ç –æ–±—É—á–µ–Ω–∏—è HitOrder –º–æ–¥–µ–ª–∏...")

    for epoch in range(1, CFG.train.epochs + 1):
        model.train()
        train_losses = []

        for xb, yb in train_loader:
            xb, yb = xb.to(device), yb.to(device).unsqueeze(1)
            optimizer.zero_grad()
            preds = model(xb)
            loss = loss_fn(preds, yb)
            loss.backward()
            optimizer.step()
            train_losses.append(loss.item())

        # –í–∞–ª–∏–¥–∞—Ü–∏—è
        model.eval()
        val_losses = []
        all_preds, all_targets = [], []

        with torch.no_grad():
            for xb, yb in val_loader:
                xb, yb = xb.to(device), yb.to(device).unsqueeze(1)
                preds = model(xb)
                loss = loss_fn(preds, yb)
                val_losses.append(loss.item())

                preds_class = (preds > 0.5).int().cpu().numpy()
                all_preds.extend(preds_class.flatten())
                all_targets.extend(yb.cpu().numpy().flatten())

        avg_train_loss = np.mean(train_losses)
        avg_val_loss = np.mean(val_losses)
        acc = accuracy_score(all_targets, all_preds)
        bal_acc = balanced_accuracy_score(all_targets, all_preds)
        f1 = f1_score(all_targets, all_preds)

        logging.info(f"üìä –≠–ø–æ—Ö–∞ {epoch}: Train Loss={avg_train_loss:.6f} | "
                     f"Val Loss={avg_val_loss:.6f} | Acc={acc:.4f} | "
                     f"Balanced Acc={bal_acc:.4f} | F1={f1:.4f}")

        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            patience_counter = 0
            torch.save(model.state_dict(), CFG.paths.hit_order_model_path)
            logging.info("üéØ –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å –æ–±–Ω–æ–≤–ª–µ–Ω–∞.")
        else:
            patience_counter += 1
            if patience_counter >= CFG.train.early_stopping_patience:
                logging.info("‚è∏ Early stopping ‚Äî –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –æ–±—É—á–µ–Ω–∏—è.")
                break

    logging.info("‚úÖ –û–±—É—á–µ–Ω–∏–µ HitOrder –º–æ–¥–µ–ª–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–æ.")

if __name__ == "__main__":
    main()
