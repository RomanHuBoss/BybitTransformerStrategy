import numpy as np
import joblib
import logging
import torch
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score
from sklearn.utils.class_weight import compute_class_weight
from collections import Counter

from feature_engineering import FeatureEngineer
from dataset import SequenceDataset, load_train_features, load_train_labels_direction
from model import DirectionalModel
from losses import CostSensitiveFocalLoss
from config import CFG

logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")

class DirectionalTrainer:
    def __init__(self):
        logging.info("üöÄ –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è Direction –º–æ–¥–µ–ª–∏")

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ
        X = load_train_features()
        y = load_train_labels_direction()

        assert len(X) == len(y), f"–î–ª–∏–Ω—ã X ({len(X)}) –∏ y ({len(y)}) –Ω–µ —Å–æ–≤–ø–∞–¥–∞—é—Ç!"
        logging.info(f"‚úÖ –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã: {len(X)} –ø—Ä–∏–º–µ—Ä–æ–≤, {X.shape[1]} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")

        # –õ–æ–≥ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤
        class_counts = Counter(y)
        total_samples = len(y)
        for label, count in class_counts.items():
            logging.info(f"üìä –ö–ª–∞—Å—Å {label}: {count} –ø—Ä–∏–º–µ—Ä–æ–≤ ({count/total_samples:.2%})")

        self.engineer = FeatureEngineer()
        self.engineer.scaler = joblib.load(CFG.paths.scaler_path)
        self.engineer.feature_columns = joblib.load(CFG.paths.feature_columns_path)

        full_dataset = SequenceDataset(X, y, CFG.labels.lookahead)
        val_size = int(len(full_dataset) * CFG.train.val_size)
        train_size = len(full_dataset) - val_size
        self.train_dataset, self.val_dataset = random_split(full_dataset, [train_size, val_size])

        self.train_loader = DataLoader(self.train_dataset, batch_size=CFG.train.batch_size, shuffle=True)
        self.val_loader = DataLoader(self.val_dataset, batch_size=CFG.train.batch_size, shuffle=False)

        model_cfg = CFG.ModelConfig()
        model_cfg.input_dim = len(self.engineer.feature_columns)
        self.model = DirectionalModel(model_cfg)

        # –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –∫–ª–∞—Å—Å–æ–≤
        class_weights = compute_class_weight(
            class_weight='balanced',
            classes=np.unique(y),
            y=y
        )
        logging.info(f"‚öñÔ∏è –í—ã—á–∏—Å–ª–µ–Ω–Ω—ã–µ –≤–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤: {class_weights}")

        self.criterion = CostSensitiveFocalLoss(alpha=torch.tensor(class_weights, dtype=torch.float32), gamma=2.0, label_smoothing=0.0)
        self.optimizer = optim.Adam(self.model.parameters(), lr=CFG.train.lr)

    def train(self):
        best_val_loss = float('inf')
        epochs_no_improve = 0
        patience = CFG.train.early_stopping_patience

        for epoch in range(CFG.train.epochs):
            self.model.train()
            total_loss = 0
            for X_batch, y_batch in self.train_loader:
                self.optimizer.zero_grad()
                output = self.model(X_batch)
                loss = self.criterion(output, y_batch)
                loss.backward()
                self.optimizer.step()
                total_loss += loss.item()

            avg_loss = total_loss / len(self.train_loader)
            val_loss, acc, bal_acc, macro_f1 = self.validate()

            logging.info(f"üßÆ –≠–ø–æ—Ö–∞ {epoch + 1}: Train Loss {avg_loss:.6f}, Val Loss {val_loss:.6f}, "
                         f"Accuracy {acc:.4f}, Balanced Acc {bal_acc:.4f}, Macro F1 {macro_f1:.4f}")

            if val_loss < best_val_loss:
                best_val_loss = val_loss
                epochs_no_improve = 0
                torch.save(self.model.state_dict(), CFG.paths.direction_model_path)
                logging.info(f"üéØ –ù–æ–≤—ã–π –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: Val Loss {val_loss:.6f}. –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞.")
            else:
                epochs_no_improve += 1

            if epochs_no_improve >= patience:
                logging.info(f"üõë –†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞: {epochs_no_improve} —ç–ø–æ—Ö –±–µ–∑ —É–ª—É—á—à–µ–Ω–∏—è.")
                break

        logging.info("‚úÖ –û–±—É—á–µ–Ω–∏–µ Direction –º–æ–¥–µ–ª–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–æ.")

    def validate(self):
        self.model.eval()
        total_loss = 0
        all_preds, all_targets = [], []
        with torch.no_grad():
            for X_batch, y_batch in self.val_loader:
                output = self.model(X_batch)
                loss = self.criterion(output, y_batch)
                total_loss += loss.item()

                preds = torch.argmax(output, dim=1).cpu().numpy()
                targets = y_batch.cpu().numpy()
                all_preds.extend(preds)
                all_targets.extend(targets)

        avg_loss = total_loss / len(self.val_loader)
        acc = accuracy_score(all_targets, all_preds)
        bal_acc = balanced_accuracy_score(all_targets, all_preds)
        macro_f1 = f1_score(all_targets, all_preds, average='macro')
        return avg_loss, acc, bal_acc, macro_f1

if __name__ == '__main__':
    trainer = DirectionalTrainer()
    trainer.train()
