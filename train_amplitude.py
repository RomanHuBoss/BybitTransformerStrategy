import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import logging
from sklearn.model_selection import train_test_split
from tqdm import tqdm

from dataset import load_train_features, load_train_labels_amplitude, AmplitudeDataset
from model import AmplitudeRegressor
from losses import AmplitudeLoss
from config import CFG

logging.basicConfig(level=logging.INFO, format='%(asctime)s [INFO] %(message)s')

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
logging.info(f"‚ö° –†–∞–±–æ—Ç–∞–µ–º –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ: {device}")

logging.info("üöÄ –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏ –∞–º–ø–ª–∏—Ç—É–¥–Ω—ã–µ –º–µ—Ç–∫–∏...")
X = load_train_features()
y = load_train_labels_amplitude()

# –†–∞–∑–¥–µ–ª—è–µ–º train/val
X_train, X_val, y_train, y_val = train_test_split(
    X, y, test_size=CFG.train.val_size, shuffle=False
)

train_dataset = AmplitudeDataset(X_train, y_train)
val_dataset = AmplitudeDataset(X_val, y_val)

train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=CFG.train.batch_size, shuffle=True)
val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=CFG.train.batch_size)

model = AmplitudeRegressor(input_size=X.shape[1]).to(device)
loss_fn = AmplitudeLoss(weights=CFG.amplitude.loss_weights, device=device)
optimizer = optim.AdamW(model.parameters(), lr=CFG.train.lr)

best_val_loss = np.inf
patience_counter = 0

logging.info("üßÆ –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è Amplitude –º–æ–¥–µ–ª–∏...")

for epoch in range(1, CFG.train.epochs + 1):
    model.train()
    train_losses = []

    for xb, yb in tqdm(train_loader, desc=f"–≠–ø–æ—Ö–∞ {epoch} [–æ–±—É—á–µ–Ω–∏–µ]", leave=False):
        xb, yb = xb.to(device), yb.to(device)
        optimizer.zero_grad()
        preds = model(xb)
        loss = loss_fn(preds, yb)
        loss.backward()
        optimizer.step()
        train_losses.append(loss.item())

    model.eval()
    val_losses = []

    with torch.no_grad():
        for xb, yb in val_loader:
            xb, yb = xb.to(device), yb.to(device)
            preds = model(xb)
            loss = loss_fn(preds, yb)
            val_losses.append(loss.item())

    avg_train_loss = np.mean(train_losses)
    avg_val_loss = np.mean(val_losses)

    logging.info(f"üìä –≠–ø–æ—Ö–∞ {epoch}: Train Loss={avg_train_loss:.6f} | Val Loss={avg_val_loss:.6f}")

    if avg_val_loss < best_val_loss - 1e-6:
        best_val_loss = avg_val_loss
        patience_counter = 0
        torch.save(model.state_dict(), str(CFG.paths.amplitude_model_path))
        logging.info("üéØ –ù–æ–≤–∞—è –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞.")
    else:
        patience_counter += 1
        if patience_counter >= CFG.train.early_stopping_patience:
            logging.info("‚è∏ Early stopping ‚Äî –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –æ–±—É—á–µ–Ω–∏—è.")
            break

logging.info("‚úÖ –û–±—É—á–µ–Ω–∏–µ Amplitude –º–æ–¥–µ–ª–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–æ.")
