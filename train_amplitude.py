import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import joblib
import logging
from sklearn.model_selection import train_test_split
from tqdm import tqdm

from dataset import load_train_features, load_train_labels_amplitude, AmplitudeDataset
from model import AmplitudeModel
from losses import AmplitudeLoss
from config import CFG

# –õ–æ–≥–≥–∏–Ω–≥
logging.basicConfig(level=logging.INFO, format='%(asctime)s [INFO] %(message)s')

# –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
logging.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")

# === –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö ===
logging.info("üöÄ –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏ –∞–º–ø–ª–∏—Ç—É–¥–Ω—ã–µ –º–µ—Ç–∫–∏...")
X = load_train_features()
y = load_train_labels_amplitude()  # –±–µ–∑ –ª–æ–≥–∞—Ä–∏—Ñ–º–∞

# –¢—Ä–µ–π–Ω/–≤–∞–ª–∏–¥–∞—Ü–∏—è —Å–ø–ª–∏—Ç
X_train, X_val, y_train, y_val = train_test_split(
    X, y, test_size=CFG.train.val_size, shuffle=False
)

# –î–∞—Ç–∞—Å–µ—Ç—ã –∏ –ª–æ–∞–¥–µ—Ä—ã
train_dataset = AmplitudeDataset(X_train, y_train)
val_dataset = AmplitudeDataset(X_val, y_val)

train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=CFG.train.batch_size, shuffle=True)
val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=CFG.train.batch_size)

# === –ú–æ–¥–µ–ª—å ===
model = AmplitudeModel(input_size=X.shape[1]).to(device)

# –õ–æ—Å—Å —Å –≤–µ—Å–∞–º–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
loss_fn = AmplitudeLoss(weights=CFG.amplitude.loss_weights, device=device)
optimizer = optim.AdamW(model.parameters(), lr=CFG.train.lr)

# Early stopping
best_val_loss = np.inf
patience_counter = 0

logging.info("üßÆ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ –∞–º–ø–ª–∏—Ç—É–¥–Ω–æ–π –º–æ–¥–µ–ª–∏...")

for epoch in range(1, CFG.train.epochs + 1):
    model.train()
    train_losses = []

    for xb, yb in tqdm(train_loader, desc=f"–≠–ø–æ—Ö–∞ {epoch} [–æ–±—É—á–µ–Ω–∏–µ]", leave=False):
        xb, yb = xb.to(device), yb.to(device)
        optimizer.zero_grad()
        preds = model(xb)
        loss = loss_fn(preds, yb)
        loss.backward()
        optimizer.step()
        train_losses.append(loss.item())

    model.eval()
    val_losses = []

    with torch.no_grad():
        for xb, yb in val_loader:
            xb, yb = xb.to(device), yb.to(device)
            preds = model(xb)
            loss = loss_fn(preds, yb)
            val_losses.append(loss.item())

    avg_train_loss = np.mean(train_losses)
    avg_val_loss = np.mean(val_losses)

    logging.info(f"üìä –≠–ø–æ—Ö–∞ {epoch}: Train Loss {avg_train_loss:.6f}, Val Loss {avg_val_loss:.6f}")

    if avg_val_loss < best_val_loss - 1e-6:
        best_val_loss = avg_val_loss
        patience_counter = 0
        torch.save(model.state_dict(), str(CFG.paths.amplitude_model_path))
        logging.info("üéØ –ù–æ–≤–∞—è –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞.")
    else:
        patience_counter += 1
        if patience_counter >= CFG.train.early_stopping_patience:
            logging.info("‚è∏ –†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞: –ø—Ä–æ–≥—Ä–µ—Å—Å –∑–∞–º–µ–¥–ª–∏–ª—Å—è.")
            break

logging.info("‚úÖ –û–±—É—á–µ–Ω–∏–µ –∞–º–ø–ª–∏—Ç—É–¥–Ω–æ–π –º–æ–¥–µ–ª–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–æ.")