import pandas as pd
import numpy as np
import torch
from torch import nn
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
import joblib
import os
import logging

from config import CFG
from model import HitOrderClassifier
from losses import FocalLoss

logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")

# Ð”Ð°Ñ‚Ð°ÑÐµÑ‚
class HitOrderDataset(Dataset):
    def __init__(self, X, y):
        self.X = torch.tensor(X, dtype=torch.float32)
        self.y = torch.tensor(y, dtype=torch.float32)

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

# Ð¢Ñ€ÐµÐ½Ð¸Ñ€Ð¾Ð²ÐºÐ° Ð¾Ð´Ð½Ð¾Ð³Ð¾ Ð¿Ñ€Ð¾Ñ„Ð¸Ð»Ñ
def train_one_profile(df, profile, feature_cols):
    sl, rr = profile
    label_col = f"hit_SL{str(sl).replace('.', '_')}_RR{str(rr).replace('.', '_')}"
    if label_col not in df.columns:
        logging.warning(f"ÐŸÑ€Ð¾Ð¿ÑƒÑÐºÐ°ÐµÐ¼ Ð¿Ñ€Ð¾Ñ„Ð¸Ð»ÑŒ {label_col}, ÐºÐ¾Ð»Ð¾Ð½ÐºÐ° Ð¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ð²ÑƒÐµÑ‚.")
        return

    logging.info(f"\nðŸš€ ÐžÐ±ÑƒÑ‡Ð°ÐµÐ¼ Ð¿Ñ€Ð¾Ñ„Ð¸Ð»ÑŒ: {label_col}")

    X = df[feature_cols].values
    y = df[label_col].values

    X_train, X_val, y_train, y_val = train_test_split(
        X, y, test_size=CFG.hitorder.val_size, random_state=42
    )

    train_ds = HitOrderDataset(X_train, y_train)
    val_ds = HitOrderDataset(X_val, y_val)

    train_loader = DataLoader(train_ds, batch_size=CFG.hitorder.batch_size, shuffle=True)
    val_loader = DataLoader(val_ds, batch_size=CFG.hitorder.batch_size, shuffle=False)

    model = HitOrderClassifier(input_dim=X.shape[1]).to(CFG.hitorder.device)
    criterion = FocalLoss(gamma=CFG.train.focal_gamma)
    optimizer = torch.optim.AdamW(model.parameters(), lr=CFG.hitorder.lr)
    best_val_loss = float('inf')

    pos_weight = (len(y) - y.sum()) / y.sum()
    logging.info(f"âœ… Ð‘Ð°Ð»Ð°Ð½Ñ ÐºÐ»Ð°ÑÑÐ¾Ð²: POS={int(y.sum())}, NEG={int(len(y) - y.sum())}, POS%={y.mean() * 100:.3f}%")
    logging.info(f"âš–ï¸ Ð Ð°ÑÑ‡ÐµÑ‚Ð½Ñ‹Ð¹ pos_weight: {pos_weight:.4f}")

    for epoch in range(CFG.hitorder.epochs):
        model.train()
        train_losses = []

        for xb, yb in train_loader:
            xb, yb = xb.to(CFG.hitorder.device), yb.to(CFG.hitorder.device)
            optimizer.zero_grad()
            out = model(xb).squeeze()
            loss = criterion(out, yb)
            loss.backward()
            optimizer.step()
            train_losses.append(loss.item())

        model.eval()
        val_losses, preds, targets = [], [], []

        with torch.no_grad():
            for xb, yb in val_loader:
                xb, yb = xb.to(CFG.hitorder.device), yb.to(CFG.hitorder.device)
                out = model(xb).squeeze()
                loss = criterion(out, yb)
                val_losses.append(loss.item())
                preds.append((out > 0.5).cpu().numpy())
                targets.append(yb.cpu().numpy())

        preds = np.concatenate(preds)
        targets = np.concatenate(targets)
        acc = (preds == targets).mean()
        val_loss = np.mean(val_losses)

        logging.info(f"ðŸ“Š Ð­Ð¿Ð¾Ñ…Ð° {epoch+1}: Train Loss={np.mean(train_losses):.6f} | Val Loss={val_loss:.6f} | Acc={acc:.4f}")

        if val_loss < best_val_loss:
            best_val_loss = val_loss
            model_path = os.path.join(CFG.paths.models_dir, f"hitorder_{label_col}.pth")
            torch.save(model.state_dict(), model_path)
            logging.info(f"ðŸŽ¯ Ð›ÑƒÑ‡ÑˆÐ°Ñ Ð¼Ð¾Ð´ÐµÐ»ÑŒ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð°: {model_path}")

# ÐžÑÐ½Ð¾Ð²Ð½Ð¾Ð¹ Ð·Ð°Ð¿ÑƒÑÐº
if __name__ == "__main__":
    df = pd.read_parquet(CFG.paths.feature_dataset_path)
    scaler = joblib.load(CFG.paths.scaler_path)
    feature_cols = joblib.load(CFG.paths.feature_columns_path)
    df[feature_cols] = scaler.transform(df[feature_cols])

    for sl in CFG.label_generation.hitorder_sl_list:
        for rr in CFG.label_generation.hitorder_rr_list:
            train_one_profile(df, (sl, rr), feature_cols)
